{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jetbrains AI Code Completion Internship task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "!{sys.executable} -m pip install q transformers\n",
    "!{sys.executable} -m pip install q torch torchvision torchaudio\n",
    "!{sys.executable} -m pip install q editdistance\n",
    "!{sys.executable} -m pip install q rouge_score\n",
    "!{sys.executable} -m pip install q sacrebleu\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset\n",
    "I had two main requirements in mind: the programming language should be popular, and the code should avoid excessive use of external libraries to ensure that the used model is capable of understanding the examples. For this reason, I selected some files from my first-year project for Object-Oriented Programming class. These files are relatively small, allowing the entire file to serve as context for each example, with the model's goal being to predict a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion_examples(file_path, num_examples=10):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    examples = []\n",
    "    length = len(lines)\n",
    "    interval = length // num_examples\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        pred = i * interval\n",
    "        prefix = ''.join(lines[0:pred])\n",
    "        middle = ''.join(lines[pred])\n",
    "        suffix = ''.join(lines[pred+1::])\n",
    "        \n",
    "        examples.append({\n",
    "            \"prefix\": prefix,\n",
    "            \"middle\": middle,\n",
    "            \"suffix\": suffix\n",
    "        })\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Diagonal.java\n",
      "Processing file: DoubleMatrix.java\n",
      "Processing file: Full.java\n",
      "All examples prepared\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "directory = 'examples'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    if os.path.isfile(file_path) and filename.endswith('.java'):\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        \n",
    "        examples += generate_completion_examples(file_path)\n",
    "\n",
    "print(\"All examples prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running experiments\n",
    "\n",
    "A model suggested in the task content, that is tiny_starcoder proved to be capable enough to draw meaningful conclusions, so I opted against changing it.\n",
    "\n",
    "In addition to the exact match metric and chrf score, I also included edit similarity that I learned about in Computational Genomics classes. This metric is actually pretty relevant to code generation, as minimizing edit distance can directly reduce amount of correction a programmer using our tool would need to make.\n",
    "\n",
    "Another metric I included is Rouge-L (Longest Common Subsequence metric). This metric proved useful, as during tests I noticed that model often generated fundamentally right predictions but added some unnecessary stuff, like extra whitespace or closing brackets. This metric recognises such cases pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Dataset saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.chrf_score import sentence_chrf\n",
    "import editdistance\n",
    "\n",
    "checkpoint = \"bigcode/tiny_starcoder_py\"\n",
    "device = \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "line_breaker = \"========\\n\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "completions = []\n",
    "\n",
    "for example in examples:\n",
    "    input_text = '<fim_prefix>' + example['prefix'] + '<fim_suffix>' + example['suffix'] + '<fim_middle>'\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=10,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    completion_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    completion_text = completion_text.replace('<|endoftext|>', '').split('<fim_middle>')[1]\n",
    "\n",
    "    em = int(example[\"middle\"] == completion_text)\n",
    "\n",
    "    edit = editdistance.eval(completion_text, example[\"middle\"])\n",
    "    max_len = max(len(completion_text), len(example[\"middle\"]))\n",
    "    edit = 1 - (edit / max_len) \n",
    "\n",
    "    chrf_score = sentence_chrf(example[\"middle\"], completion_text)\n",
    "    rouge_l = scorer.score(example['middle'], completion_text)['rougeL'].fmeasure\n",
    "    \n",
    "    completions.append({\n",
    "        \"Prefix\": example[\"prefix\"],\n",
    "        \"Suffix\": example[\"suffix\"],\n",
    "        \"Expected Middle\": example[\"middle\"],\n",
    "        \"Model Completion\": completion_text,\n",
    "        \"Exact Match\": em,\n",
    "        \"Edit Similarity\": edit,\n",
    "        \"CHRF Score\": chrf_score,\n",
    "        \"ROUGE-L\": rouge_l\n",
    "    })\n",
    "\n",
    "print(len(completions))\n",
    "df = pd.DataFrame(completions)\n",
    "\n",
    "df.to_csv(\"completions.csv\", index=False)\n",
    "\n",
    "print(\"Dataset saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing conclusions\n",
    "I used Pearson correlation coefficient to assess the correlation between human judgement and the model's automated metrics. Edit similarity, CHRF score and ROUGE-L are all closely matched, while exact matching is obviously not really precise as it doesn't differentiate between bad and partially good predictions. That being said, don't believe our dataset is large enough to confidently say which of the proposed metrics captures human judgement the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with My Score:\n",
      "Exact Match        0.309423\n",
      "Edit Similarity    0.867961\n",
      "CHRF Score         0.876278\n",
      "ROUGE-L            0.931328\n",
      "Name: My Score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"completions_labeled.csv\")\n",
    "numeric_df = df[['Exact Match', 'Edit Similarity', 'CHRF Score', 'ROUGE-L', 'My Score']]\n",
    "\n",
    "correlations = numeric_df.corr()['My Score'][['Exact Match', 'Edit Similarity', 'CHRF Score', 'ROUGE-L']]\n",
    "\n",
    "print(\"Correlation with My Score:\")\n",
    "print(correlations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
